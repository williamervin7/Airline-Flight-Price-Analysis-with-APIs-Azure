{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17END5AbXeEv2qKb_ocFXz2Ozyne6Eu6u",
      "authorship_tag": "ABX9TyNHjeMwEVLhT3mUmbNhtML7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üõ´ Flight Price Forecasting Pipeline\n",
        "\n",
        "## üéØ Project Goal\n",
        "\n",
        "The primary objective of this notebook is to develop a **robust machine learning model** capable of predicting flight prices for specific routes. By accurately forecasting the expected price of a flight, we can detect **\"deals\"** (offers priced significantly below the predicted market rate) and **\"overpriced\"** listings, providing actionable advice to users on whether to **Buy Now** or **Wait**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Data Overview\n",
        "\n",
        "The raw data consists of multiple daily scrapes from the of flight itinerary information. Crucially, the raw data is at the **flight segment level**, meaning multi-connection journeys appear as multiple rows.\n",
        "\n",
        "* **Source:** Aggregated daily flight offers from Amademus API.\n",
        "* **Key Challenge:** The price landscape is highly **volatile**. Prices change frequently due to dynamic pricing algorithms, load factors, and time-to-departure. Our feature engineering must account for this volatility.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Methodology Overview\n",
        "\n",
        "The pipeline uses a multi-stage approach, leveraging advanced **time-series feature engineering** and **Gradient Boosting** models (LightGBM) to capture the complex patterns in flight price behavior.\n",
        "\n",
        "### 1. Data Cleaning (Segment Consolidation)\n",
        "\n",
        "The raw, segment-level data is aggregated into a single, clean record per unique flight **journey** (`OfferID`). This forms the base dataset for all subsequent steps.\n",
        "\n",
        "### 2. Time-Series Feature Engineering\n",
        "\n",
        "This is the most critical stage. We create **lagged price features** and **rolling statistics** (mean, standard deviation) by tracking the history of the *minimum price* for a specific flight date. This converts noisy daily price observations into stable, predictive time-series signals.\n",
        "\n",
        "### 3. Model Training & Validation\n",
        "\n",
        "A LightGBM Regressor is trained using **Time Series Cross-Validation (TSCV)**. TSCV ensures that the model is only ever trained on historical data, validating its performance on future data in a manner that realistically simulates deployment.\n",
        "\n",
        "### 4. Deal Detection\n",
        "\n",
        "The final model predictions are used to calculate the **percentage error** between the actual price and the predicted price. Thresholds are applied to categorize offers and generate concrete recommendations:\n",
        "\n",
        "| Prediction Error % | Recommendation | Category |\n",
        "| :--- | :--- | :--- |\n",
        "| **$\\leq -12\\%$** | üî• BUY NOW | **EXCELLENT** |\n",
        "| **$-12\\%$ to $-7\\%$** | ‚úÖ STRONG BUY | **GOOD** |\n",
        "| **$> 10\\%$** | ‚ùå WAIT | **OVERPRICED** |\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Pipeline Structure\n",
        "\n",
        "This notebook is structured around the following functions, which represent the sequential steps of the ML workflow:\n",
        "\n",
        "* `get_data()`: Loads all raw CSVs.\n",
        "* `clean_raw_data()`: Consolidates segments into journeys.\n",
        "* `filter_target_route()`: Focuses on specific routes (e.g., IAH -> LAX).\n",
        "* `engineer_features()`: Creates standard temporal and flight features.\n",
        "* `create_price_history_features()`: Generates lag and rolling price features.\n",
        "* `train_price_model()`: Fits the LightGBM model with TSCV.\n",
        "* `predict_prices()`: Generates predictions on the data.\n",
        "* `evaluate_model()`: Reports performance metrics (MAE, MAPE, R¬≤).\n",
        "* `detect_deals()`: Final classification and recommendation step.\n",
        "* `run_complete_pipeline()`: Orchestrates all steps."
      ],
      "metadata": {
        "id": "E5wQWzWAs6f5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hCegFZ6PP1n",
        "outputId": "13d4129a-d552-4dbd-ce89-c52dbd012e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Airline-Flight-Price-Analysis-with-APIs-Azure'...\n",
            "remote: Enumerating objects: 541, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 541 (delta 120), reused 17 (delta 13), pack-reused 394 (from 1)\u001b[K\n",
            "Receiving objects: 100% (541/541), 1.43 MiB | 11.25 MiB/s, done.\n",
            "Resolving deltas: 100% (346/346), done.\n",
            "data  figures  notebooks  README.md  scripts\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "import io\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
        "import lightgbm as lgb\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Go to the content folder\n",
        "%cd /content\n",
        "\n",
        "# Remove old repo if it exists\n",
        "!rm -rf Airline-Flight-Price-Analysis-with-APIs-Azure\n",
        "\n",
        "# Clone the latest version of your repo\n",
        "!git clone https://github.com/williamervin7/Airline-Flight-Price-Analysis-with-APIs-Azure.git\n",
        "\n",
        "# Check contents\n",
        "!ls Airline-Flight-Price-Analysis-with-APIs-Azure\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "data_path = 'Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the formatted Markdown for the data loading step:\n",
        "\n",
        "# üì• STEP 1: Data Ingestion\n",
        "\n",
        "This step is responsible for loading the raw flight segment data from the file system. Since the data is stored across multiple CSV files (one for each daily scrape), the `get_data` function consolidates them into a single, unified DataFrame for processing.\n",
        "\n",
        "-----\n",
        "\n",
        "## `get_data` Function\n",
        "\n",
        "The function performs the following critical tasks:\n",
        "\n",
        "1.  **Iterative Loading:** Reads every CSV file in the specified directory path.\n",
        "2.  **Date Parsing:** Automatically converts critical date-time columns (like `'Departure'`, `'Arrival'`, `'DepartureDate'`, and `'SearchDate'`) into the correct datetime format upon load.\n",
        "3.  **Consolidation:** Uses `pd.concat` to merge all individual daily files into one master DataFrame."
      ],
      "metadata": {
        "id": "pl5wTgSoubpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads and consolidates all CSV files from a specified directory into a single Pandas DataFrame.\n",
        "\n",
        "    This function iterates through all files in the given directory, loads only\n",
        "    those ending with '.csv', and automatically parses the relevant date columns\n",
        "    into datetime objects during loading.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : str\n",
        "        The full path to the directory containing the raw CSV data files.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        A single, concatenated DataFrame containing all rows from all CSV files\n",
        "        found in the directory.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The function assumes that all CSV files have the following columns which\n",
        "    should be parsed as dates: 'Departure', 'Arrival', 'DepartureDate', and\n",
        "    'SearchDate'. Prints a summary of the files loaded and the total number of rows.\n",
        "    \"\"\"\n",
        "    # Get list of files in folder\n",
        "    files = sorted(os.listdir(data_path))\n",
        "\n",
        "    # Collect all dataframes\n",
        "    dfs = []\n",
        "\n",
        "    for f in files:\n",
        "      if f.endswith('.csv'): #only load CSV\n",
        "        full_path = os.path.join(data_path, f)\n",
        "        print(f\"Loading {full_path}...\")\n",
        "\n",
        "        df = pd.read_csv(full_path, parse_dates=['Departure', 'Arrival', 'DepartureDate', 'SearchDate'])\n",
        "        dfs.append(df)\n",
        "\n",
        "    all_data = pd.concat(dfs, ignore_index=True)\n",
        "    print(f'Loaded {len(files)-1} files, {all_data.shape[0]} rows')\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "vXudwjn1qumy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßπ STEP 2: Data Preparation (Cleaning & Filtering)\n",
        "\n",
        "Before we can build any features, the raw data must be structured correctly. This step addresses two critical pre-processing requirements: **data granularity correction** and **route focus**.\n",
        "\n",
        "## 1\\. Segment Consolidation (`clean_raw_data`)\n",
        "\n",
        "The raw data is at the **flight segment level**, meaning a connecting flight (e.g., IAH ‚Üí DEN ‚Üí LAX) appears as two separate rows, but both share the same `OfferID` and overall `Price`. This step aggregates these segments into a single row representing the complete journey.\n",
        "\n",
        "### Key Aggregation Logic:\n",
        "\n",
        "  * **Grouping Key:** Unique combination of (`OfferID`, `DepartureDate`, `SearchDate`).\n",
        "  * **Journey Origin/Destination:** Taken from the `From` of the first segment and the `To` of the last segment (after sorting by departure time).\n",
        "  * **Total Duration:** Calculated as the time difference between the first segment's departure and the last segment's arrival.\n",
        "  * **New Features:** `num_segments`, `is_direct`, `primary_airline`, and `stops` are created to capture the journey's characteristics.\n",
        "\n",
        "### Output Snapshot\n",
        "\n",
        "| Feature | Description | Example (2-Segment Journey) |\n",
        "| :--- | :--- | :--- |\n",
        "| **`origin`** | Departure airport of first flight. | `IAH` |\n",
        "| **`destination`** | Arrival airport of last flight. | `LAX` |\n",
        "| **`price`** | Single price for the entire itinerary. | `150.90` |\n",
        "| **`total_duration_minutes`** | End-to-end travel time. | `360` |\n",
        "| **`num_segments`** | Number of legs (flights). | `2` |\n",
        "| **`stops`** | Airports visited between origin and destination. | `DEN` |\n",
        "\n",
        "-----\n",
        "\n",
        "## 2\\. Route Filtering (`filter_target_route`)\n",
        "\n",
        "To make the modeling problem tractable and focus on key business insights, the data is filtered to include only specific routes. For this project, we are focusing on flights originating from **Houston (IAH)** destined for airports in the **Greater Los Angeles Area (LAX, ONT)**.\n"
      ],
      "metadata": {
        "id": "zRsmBE4Lav0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_raw_data(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleans raw flight segment data by consolidating multi-segment itineraries\n",
        "    into a single row per unique offer.\n",
        "\n",
        "    The raw data is assumed to have multiple rows for connecting flights\n",
        "    sharing the same OfferID, DepartureDate, and SearchDate. This function\n",
        "    aggregates flight segment details to represent the entire journey.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw DataFrame containing one row per flight segment.\n",
        "        Expected columns include 'OfferID', 'DepartureDate', 'SearchDate',\n",
        "        'Departure', 'Arrival', 'Price', 'From', 'To', 'Airline', 'Flight'.\n",
        "    verbose : bool, optional\n",
        "        If True, prints progress and summary statistics. The default is True.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        A cleaned DataFrame where each row represents a complete flight offer\n",
        "        (journey), ready for feature engineering.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The output columns use standard Python snake_case for consistency.\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"STEP 1: DATA CLEANING\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Raw data shape: {df.shape}\")\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Parse dates\n",
        "    df['Departure'] = pd.to_datetime(df['Departure'])\n",
        "    df['Arrival'] = pd.to_datetime(df['Arrival'])\n",
        "    df['DepartureDate'] = pd.to_datetime(df['DepartureDate'])\n",
        "    df['SearchDate'] = pd.to_datetime(df['SearchDate'])\n",
        "\n",
        "    # Group by OfferID to consolidate connecting flights\n",
        "    offer_groups = []\n",
        "\n",
        "    for (offer_id, dep_date, search_date), group in df.groupby(['OfferID', 'DepartureDate', 'SearchDate']):\n",
        "        # Sort by departure time to get journey order\n",
        "        group = group.sort_values('Departure')\n",
        "\n",
        "        # Get journey details\n",
        "        first_flight = group.iloc[0]\n",
        "        last_flight = group.iloc[-1]\n",
        "\n",
        "        # Calculate total journey time\n",
        "        total_duration_minutes = (last_flight['Arrival'] - first_flight['Departure']).total_seconds() / 60\n",
        "\n",
        "        # Consolidate offer info\n",
        "        offer_info = {\n",
        "            'offer_id': offer_id,\n",
        "            'search_date': search_date,\n",
        "            'departure_date': dep_date,\n",
        "            'price': first_flight['Price'],  # Price is same for all segments\n",
        "\n",
        "            # Origin and destination\n",
        "            'origin': first_flight['From'],\n",
        "            'destination': last_flight['To'],\n",
        "\n",
        "            # Timing\n",
        "            'departure_time': first_flight['Departure'],\n",
        "            'arrival_time': last_flight['Arrival'],\n",
        "            'total_duration_minutes': total_duration_minutes,\n",
        "\n",
        "            # Flight details\n",
        "            'num_segments': len(group),\n",
        "            'is_direct': len(group) == 1,\n",
        "            'airlines': '|'.join(group['Airline'].unique()),\n",
        "            'primary_airline': group['Airline'].iloc[0],\n",
        "            'flight_numbers': '|'.join(group['Flight'].astype(str).values),\n",
        "\n",
        "            # Intermediate stops\n",
        "            'stops': '|'.join(group['To'].iloc[:-1].values) if len(group) > 1 else 'DIRECT',\n",
        "        }\n",
        "\n",
        "        offer_groups.append(offer_info)\n",
        "\n",
        "    # Create cleaned dataframe\n",
        "    cleaned_df = pd.DataFrame(offer_groups)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nCleaned data shape: {cleaned_df.shape}\")\n",
        "        print(f\"Unique offers: {cleaned_df['offer_id'].nunique()}\")\n",
        "        print(f\"Price range: ${cleaned_df['price'].min():.2f} - ${cleaned_df['price'].max():.2f}\")\n",
        "        print(f\"Date range: {cleaned_df['search_date'].min().date()} to {cleaned_df['search_date'].max().date()}\")\n",
        "        print(f\"\\nFlight types:\")\n",
        "        print(f\"  Direct flights: {cleaned_df['is_direct'].sum()} ({cleaned_df['is_direct'].mean()*100:.1f}%)\")\n",
        "        print(f\"  Connecting flights: {(~cleaned_df['is_direct']).sum()} ({(~cleaned_df['is_direct']).mean()*100:.1f}%)\")\n",
        "        print(f\"\\nAirlines:\")\n",
        "        print(cleaned_df['primary_airline'].value_counts())\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "def filter_target_route(df: pd.DataFrame, origin: str = 'IAH',\n",
        "                       destinations: List[str] = ['LAX', 'ONT'],\n",
        "                       verbose: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the flight offers DataFrame to include only specific origin-destination routes.\n",
        "\n",
        "    This function is used to focus the analysis and modeling efforts on a\n",
        "    pre-defined set of routes, such as flights originating from IAH and\n",
        "    destined for the Los Angeles area (LAX, ONT, etc.).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The input DataFrame containing consolidated flight offer data.\n",
        "        Must contain 'origin' and 'destination' columns.\n",
        "    origin : str, optional\n",
        "        The specific origin airport code (e.g., 'IAH') to filter on. The default is 'IAH'.\n",
        "    destinations : List[str], optional\n",
        "        A list of destination airport codes (e.g., ['LAX', 'ONT']) to filter on.\n",
        "        The default is ['LAX', 'ONT'].\n",
        "    verbose : bool, optional\n",
        "        If True, prints a summary of the filtering operation, including the\n",
        "        data count before and after, and a destination breakdown. The default is True.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        A new DataFrame containing only the flight offers matching the\n",
        "        specified origin and destination criteria.\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"STEP 2: ROUTE FILTERING\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "    # Filter for IAH to LAX/ONT area\n",
        "    mask = (df['origin'] == origin) & (df['destination'].isin(destinations))\n",
        "    filtered_df = df[mask].copy()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Filtering: {origin} ‚Üí {destinations}\")\n",
        "        print(f\"Before: {len(df)} offers\")\n",
        "        print(f\"After: {len(filtered_df)} offers\")\n",
        "        print(f\"\\nDestination breakdown:\")\n",
        "        print(filtered_df['destination'].value_counts())\n",
        "\n",
        "    return filtered_df"
      ],
      "metadata": {
        "id": "zGv-wMOb31Sb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üí° STEP 3 & 4: Feature Engineering\n",
        "\n",
        "Feature Engineering is the most critical stage of this pipeline, as it transforms raw date and flight details into predictive signals. This process is divided into two phases: **Standard Features** (temporal, duration, categorical) and **Time-Series Price History Features**.\n",
        "\n",
        "-----\n",
        "\n",
        "## 3\\. Standard Feature Engineering (`engineer_features`)\n",
        "\n",
        "This function creates foundational features that capture the seasonality, timing, and structural aspects of the flight offers.\n",
        "\n",
        "### Feature Categories\n",
        "\n",
        "| Category | Features | Description |\n",
        "| :--- | :--- | :--- |\n",
        "| **Booking Window** | `days_until_departure`, `weeks_until_departure` | Quantifies the advance purchase time, which is highly correlated with price. |\n",
        "| **Temporal Seasonality** | `departure_month`, `departure_day_of_week`, `search_day_of_week` | Captures monthly seasonality and day-of-week effects (e.g., cheaper to search on Tuesday). |\n",
        "| **Departure Time** | `departure_hour`, `is_morning`, `is_evening`, `is_afternoon` | Captures price differences based on time of day (e.g., morning/evening flights are often pricier). |\n",
        "| **Booking Flags** | `is_last_minute`, `is_early_booker` | Categorical flags for important booking windows (e.g., booking within 7 days). |\n",
        "| **Flight Structure** | `duration_hours`, `num_stops`, `is_direct_flight` | Structural characteristics that determine the flight's inherent cost/value. |\n",
        "| **Categorical Encoding** | `airline_encoded`, `is_lax` | Numerical mapping for airlines (using simple integer encoding) and one-hot encoding for specific destinations. |\n",
        "\n",
        "### Code\n",
        "\n",
        "```python\n",
        "def engineer_features(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n",
        "    # ... implementation details\n",
        "    return df\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "## üìà 4. Time-Series Price History Features (`create_price_history_features`)\n",
        "\n",
        "Price forecasting requires understanding the trajectory of the price leading up to the current search date. These features are generated using a **time-series approach** to prevent data leakage.\n",
        "\n",
        "### Key Features (Generated per Unique Flight Itinerary)\n",
        "\n",
        "| Feature | Calculation Method | Prediction Goal |\n",
        "| :--- | :--- | :--- |\n",
        "| **`price_lag_1`** | Price observed on the search date immediately prior (`.shift(1)`). | Captures the **previous day's price level**. |\n",
        "| **`price_rolling_mean_3`** | Average price over the past 3 days (excluding today). | Provides a stable measure of the **recent price trend**. |\n",
        "| **`price_rolling_std_3`** | Standard deviation over the past 3 days (excluding today). | Measures **price volatility**‚Äîa high std often precedes sharp changes. |\n",
        "| **`price_min_last_7`** | Minimum price observed over the past 7 days. | Captures the **best deal** seen recently for this flight. |\n",
        "| **`price_change_1d`** | Price difference between today and yesterday (`.diff(1)`). | Measures **price momentum** (is the price rising or falling?). |\n",
        "\n",
        "### Data Leakage Prevention\n",
        "\n",
        "The core of this step involves sorting the data by `search_date` **within each unique flight** and using the `.shift(1)` operation. This ensures that the price observed *today* is only predicted using information known *yesterday or earlier*.\n",
        "\n",
        "### Code\n",
        "\n",
        "```python\n",
        "def create_price_history_features(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n",
        "    # ... implementation details (Groups, Sorts, Shifts)\n",
        "    return df\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "## üìú Complete Feature List (`get_feature_list`)\n",
        "\n",
        "The model training step will utilize all the following engineered features, combining structural, temporal, and critical time-series information:\n",
        "\n",
        "```python\n",
        "[\n",
        "    # Booking window\n",
        "    'days_until_departure', 'weeks_until_departure',\n",
        "    \n",
        "    # Time features\n",
        "    'search_day_of_week', 'departure_day_of_week', 'departure_hour', 'departure_month',\n",
        "    \n",
        "    # Categorical Flags\n",
        "    'is_weekend_departure', 'is_friday_departure', 'is_monday_departure',\n",
        "    'is_morning', 'is_afternoon', 'is_evening', 'is_last_minute', 'is_early_booker',\n",
        "    \n",
        "    # Flight characteristics\n",
        "    'duration_hours', 'is_direct_flight', 'num_stops', 'is_lax', 'airline_encoded',\n",
        "    \n",
        "    # Price history (Time-Series Features)\n",
        "    'price_lag_1', 'price_lag_3', 'price_rolling_mean_3', 'price_rolling_std_3',\n",
        "    'price_min_last_7', 'price_max_last_7', 'price_change_1d',\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "r693MtdgvXPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def engineer_features(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a comprehensive set of predictive features from the cleaned flight offers DataFrame.\n",
        "\n",
        "    Features include temporal components (day of week, month), booking window metrics,\n",
        "    flight characteristics (duration, stops), and one-hot style categorical flags\n",
        "    to capture price seasonality and patterns.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The cleaned DataFrame containing one row per flight offer, expected to\n",
        "        have columns like 'search_date', 'departure_date', 'departure_time',\n",
        "        'total_duration_minutes', 'is_direct', 'num_segments', 'destination',\n",
        "        and 'primary_airline'.\n",
        "    verbose : bool, optional\n",
        "        If True, prints a summary of the engineered features and booking window\n",
        "        distribution. The default is True.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The DataFrame augmented with new, model-ready features, including:\n",
        "        - Temporal features (e.g., 'departure_day_of_week', 'search_month').\n",
        "        - Booking window features (e.g., 'days_until_departure', 'is_last_minute').\n",
        "        - Flight characteristics (e.g., 'duration_hours', 'num_stops').\n",
        "        - Encoded categorical variables ('airline_encoded', 'is_lax', 'is_ont').\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"STEP 3: FEATURE ENGINEERING\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # === BOOKING WINDOW FEATURES ===\n",
        "    df['days_until_departure'] = (df['departure_date'] - df['search_date']).dt.days\n",
        "    df['weeks_until_departure'] = df['days_until_departure'] / 7\n",
        "\n",
        "    # === TIME FEATURES ===\n",
        "    # Search date features\n",
        "    df['search_day_of_week'] = df['search_date'].dt.dayofweek  # 0=Mon, 6=Sun\n",
        "    df['search_day_of_month'] = df['search_date'].dt.day\n",
        "    df['search_week_of_year'] = df['search_date'].dt.isocalendar().week\n",
        "    df['search_month'] = df['search_date'].dt.month\n",
        "\n",
        "    # Departure date features\n",
        "    df['departure_day_of_week'] = df['departure_date'].dt.dayofweek\n",
        "    df['departure_day_of_month'] = df['departure_date'].dt.day\n",
        "    df['departure_week_of_year'] = df['departure_date'].dt.isocalendar().week\n",
        "    df['departure_month'] = df['departure_date'].dt.month\n",
        "    df['departure_hour'] = df['departure_time'].dt.hour\n",
        "\n",
        "    # === CATEGORICAL FEATURES ===\n",
        "    df['is_weekend_departure'] = df['departure_day_of_week'].isin([5, 6]).astype(int)\n",
        "    df['is_friday_departure'] = (df['departure_day_of_week'] == 4).astype(int)\n",
        "    df['is_monday_departure'] = (df['departure_day_of_week'] == 0).astype(int)\n",
        "\n",
        "    # Time of day\n",
        "    df['is_early_morning'] = (df['departure_hour'] < 6).astype(int)  # Red-eye\n",
        "    df['is_morning'] = ((df['departure_hour'] >= 6) & (df['departure_hour'] < 12)).astype(int)\n",
        "    df['is_afternoon'] = ((df['departure_hour'] >= 12) & (df['departure_hour'] < 18)).astype(int)\n",
        "    df['is_evening'] = (df['departure_hour'] >= 18).astype(int)\n",
        "\n",
        "    # Booking patterns\n",
        "    df['is_last_minute'] = (df['days_until_departure'] <= 7).astype(int)\n",
        "    df['is_early_booker'] = (df['days_until_departure'] >= 30).astype(int)\n",
        "    df['is_moderate_advance'] = ((df['days_until_departure'] > 7) &\n",
        "                                  (df['days_until_departure'] < 30)).astype(int)\n",
        "\n",
        "    # === FLIGHT CHARACTERISTICS ===\n",
        "    df['duration_hours'] = df['total_duration_minutes'] / 60\n",
        "    df['is_direct_flight'] = df['is_direct'].astype(int)\n",
        "    df['num_stops'] = df['num_segments'] - 1\n",
        "\n",
        "    # Destination encoding (LAX vs ONT)\n",
        "    df['is_lax'] = (df['destination'] == 'LAX').astype(int)\n",
        "    df['is_ont'] = (df['destination'] == 'ONT').astype(int)\n",
        "\n",
        "    # Airline encoding\n",
        "    airline_map = {airline: idx for idx, airline in enumerate(df['primary_airline'].unique())}\n",
        "    df['airline_encoded'] = df['primary_airline'].map(airline_map)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nFeatures created: {len([c for c in df.columns if c.startswith(('is_', 'days_', 'weeks_', 'num_', 'duration_'))])} engineered features\")\n",
        "        print(f\"\\nBooking window distribution:\")\n",
        "        print(f\"  Last minute (‚â§7 days): {df['is_last_minute'].sum()}\")\n",
        "        print(f\"  Moderate (8-29 days): {df['is_moderate_advance'].sum()}\")\n",
        "        print(f\"  Early (‚â•30 days): {df['is_early_booker'].sum()}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_price_history_features(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates time-series based price history features (lags, rolling stats, momentum)\n",
        "    for each unique flight itinerary (Departure Date, Departure Hour, Destination).\n",
        "\n",
        "    These features are crucial for price forecasting as they capture the temporal\n",
        "    trend, volatility, and momentum of the price leading up to the search date.\n",
        "    The calculations are performed sequentially based on the 'search_date' to prevent\n",
        "    data leakage.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The input DataFrame, which must contain the 'price' column and the\n",
        "        grouping columns: 'departure_date', 'departure_hour', and 'destination'.\n",
        "        It is assumed this DataFrame is already grouped/filtered by route.\n",
        "    verbose : bool, optional\n",
        "        If True, prints a header and summary statistics about the created features.\n",
        "        The default is True.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The original DataFrame augmented with columns representing lagged prices,\n",
        "        rolling means/standard deviations, and price changes.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - **Time Ordering:** Data is sorted by `search_date` within each unique flight\n",
        "      to ensure correct sequential calculation of lags and rolling statistics.\n",
        "    - **Data Leakage Prevention:** The `.shift(1)` operation is used before\n",
        "      calculating rolling statistics to ensure the current day's price is *not* used to predict itself.\n",
        "    - **Imputation:** Missing values (NaNs) are filled using the overall dataset\n",
        "      mean for price levels and 0 for price change/standard deviation features.\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"STEP 4: PRICE HISTORY FEATURES\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['departure_date', 'departure_hour', 'search_date'])\n",
        "\n",
        "    # Group by unique flight (departure date + time + destination)\n",
        "    for (dep_date, dep_hour, dest), group in df.groupby(['departure_date', 'departure_hour', 'destination']):\n",
        "        idx = group.index\n",
        "\n",
        "        if len(group) > 1:\n",
        "            # Lag features - what was the price yesterday, 3 days ago, etc.\n",
        "            df.loc[idx, 'price_lag_1'] = group['price'].shift(1)\n",
        "            df.loc[idx, 'price_lag_3'] = group['price'].shift(3)\n",
        "            df.loc[idx, 'price_lag_7'] = group['price'].shift(7)\n",
        "\n",
        "            # Rolling statistics\n",
        "            df.loc[idx, 'price_rolling_mean_3'] = group['price'].shift(1).rolling(window=3, min_periods=1).mean()\n",
        "            df.loc[idx, 'price_rolling_std_3'] = group['price'].shift(1).rolling(window=3, min_periods=1).std()\n",
        "            df.loc[idx, 'price_min_last_7'] = group['price'].shift(1).rolling(window=7, min_periods=1).min()\n",
        "            df.loc[idx, 'price_max_last_7'] = group['price'].shift(1).rolling(window=7, min_periods=1).max()\n",
        "\n",
        "            # Price momentum (is it trending up or down?)\n",
        "            df.loc[idx, 'price_change_1d'] = group['price'].diff(1)\n",
        "            df.loc[idx, 'price_change_3d'] = group['price'].diff(3)\n",
        "\n",
        "    # Fill NaN with reasonable defaults\n",
        "    overall_mean = df['price'].mean()\n",
        "    overall_std = df['price'].std()\n",
        "\n",
        "    lag_cols = ['price_lag_1', 'price_lag_3', 'price_lag_7',\n",
        "                'price_rolling_mean_3', 'price_min_last_7', 'price_max_last_7']\n",
        "\n",
        "    for col in lag_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(overall_mean)\n",
        "        else:\n",
        "            df[col] = overall_mean\n",
        "\n",
        "    # Fill std and changes with 0\n",
        "    if 'price_rolling_std_3' in df.columns:\n",
        "        df['price_rolling_std_3'] = df['price_rolling_std_3'].fillna(overall_std)\n",
        "    else:\n",
        "        df['price_rolling_std_3'] = overall_std\n",
        "\n",
        "    for col in ['price_change_1d', 'price_change_3d']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(0)\n",
        "        else:\n",
        "            df[col] = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Price history features created\")\n",
        "        print(f\"Average price: ${overall_mean:.2f}\")\n",
        "        print(f\"Price std dev: ${overall_std:.2f}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_feature_list() -> List[str]:\n",
        "    \"\"\"\n",
        "    Returns a predefined list of feature names to be used for model training.\n",
        "\n",
        "    These features are grouped into several categories essential for predicting\n",
        "    flight prices: booking characteristics, temporal seasonality, flight\n",
        "    attributes, and critical price momentum history.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[str]\n",
        "        A list of string names corresponding to the columns in the engineered\n",
        "        DataFrame that should be used as input features (X) for the model.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        # Booking window\n",
        "        'days_until_departure',\n",
        "        'weeks_until_departure',\n",
        "\n",
        "        # Time features\n",
        "        'search_day_of_week',\n",
        "        'departure_day_of_week',\n",
        "        'departure_hour',\n",
        "        'departure_month',\n",
        "\n",
        "        # Categorical\n",
        "        'is_weekend_departure',\n",
        "        'is_friday_departure',\n",
        "        'is_monday_departure',\n",
        "        'is_morning',\n",
        "        'is_afternoon',\n",
        "        'is_evening',\n",
        "        'is_last_minute',\n",
        "        'is_early_booker',\n",
        "\n",
        "        # Flight characteristics\n",
        "        'duration_hours',\n",
        "        'is_direct_flight',\n",
        "        'num_stops',\n",
        "        'is_lax',\n",
        "        'airline_encoded',\n",
        "\n",
        "        # Price history\n",
        "        'price_lag_1',\n",
        "        'price_lag_3',\n",
        "        'price_rolling_mean_3',\n",
        "        'price_rolling_std_3',\n",
        "        'price_min_last_7',\n",
        "        'price_max_last_7',\n",
        "        'price_change_1d',\n",
        "    ]"
      ],
      "metadata": {
        "id": "0B2M8umsmLXF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† STEP 5: Model Training and Cross-Validation\n",
        "\n",
        "The `train_price_model` function is the core of the forecasting engine. It selects a powerful **ensemble model** (defaulting to LightGBM), configures its hyperparameters, and ensures robust training using a dedicated time-series validation technique.\n",
        "\n",
        "-----\n",
        "\n",
        "## üöÄ Time Series Cross-Validation (TSCV)\n",
        "\n",
        "Since flight price data is a time-series, standard $k$-fold cross-validation is inappropriate as it would allow the model to train on **future** data to predict the **past**, leading to data leakage and overly optimistic scores.\n",
        "\n",
        "We use **TimeSeriesSplit**  to ensure strict temporal separation:\n",
        "\n",
        "1.  Each validation fold is always **chronologically later** than its corresponding training fold.\n",
        "2.  The training set grows with each subsequent fold.\n",
        "3.  This methodology accurately simulates real-world deployment, where the model must generalize to unseen, future data.\n",
        "\n",
        "## üõ†Ô∏è Model and Training Details\n",
        "\n",
        "| Component | Detail | Rationale |\n",
        "| :--- | :--- | :--- |\n",
        "| **Model Type** | **LightGBM Regressor** (Default) | Chosen for its speed, efficiency, and strong performance on heterogeneous, high-dimensional data, outperforming standard Scikit-learn models like `RandomForestRegressor`. |\n",
        "| **Hyperparameters** | `n_estimators=150`, `learning_rate=0.05`, `max_depth=6` | A balanced set of parameters optimized for performance without severe overfitting. |\n",
        "| **Data Handling** | The feature data (`X`) is ensured to be sorted by `search_date` for correct TSCV splits. Remaining NaN values are imputed with the column mean. | Ensures the temporal sequence is maintained and prevents crashes due to residual missing data. |\n",
        "| **Metrics** | **MAE, RMSE, MAPE, and R¬≤** | Standard regression metrics are calculated for both in-sample (training) and out-of-sample (CV average) performance to check for overfitting. |\n",
        "\n",
        "### Code\n",
        "\n",
        "```python\n",
        "def train_price_model(df: pd.DataFrame, model_type: str = 'lightgbm', verbose: bool = True) -> Dict[str, Any]:\n",
        "    # ... implementation details\n",
        "    return model_artifacts\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "## üìà Feature Importance (LightGBM)\n",
        "\n",
        "A key advantage of using a tree-based model like LightGBM is the ability to analyze feature importance. The results consistently show that the **Time-Series Price History Features** are the most predictive of the final price.\n",
        "\n",
        "| Rank | Feature Name | Description |\n",
        "| :--- | :--- | :--- |\n",
        "| **1.** | `price_lag_1` | The price observed yesterday is the best predictor of today's price. |\n",
        "| **2.** | `price_rolling_mean_3` | The recent average price trend. |\n",
        "| **3.** | `days_until_departure` | How far in advance the flight is booked (critical factor). |\n",
        "| **4.** | `price_rolling_std_3` | The volatility in the price over the last 3 days. |\n",
        "| **5.** | `departure_hour` | Price variations based on the time of day the flight leaves. |\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "qiJgxgUqvuVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_price_model(df: pd.DataFrame, model_type: str = 'lightgbm',\n",
        "                     verbose: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Trains a regression model to predict flight prices using time-series cross-validation.\n",
        "\n",
        "    This function prepares the data, selects and configures a machine learning model\n",
        "    (LightGBM, Gradient Boosting, or Random Forest), trains it using a temporal\n",
        "    split to simulate real-world conditions, evaluates its performance, and returns\n",
        "    the trained model and performance artifacts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The fully engineered DataFrame containing the 'price' column (target) and\n",
        "        all necessary feature columns generated in previous steps.\n",
        "    model_type : str, optional\n",
        "        The type of model to train. Must be one of 'lightgbm', 'gradient_boosting',\n",
        "        or 'random_forest'. The default is 'lightgbm'.\n",
        "    verbose : bool, optional\n",
        "        If True, prints detailed training statistics, cross-validation scores for\n",
        "        each fold, final performance metrics, and the top feature importances.\n",
        "        The default is True.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing the trained model object and various artifacts:\n",
        "        'model' (The fitted model object),\n",
        "        'feature_cols' (List of features used),\n",
        "        'cv_metrics' (Average out-of-sample metrics),\n",
        "        'train_metrics' (In-sample training metrics),\n",
        "        'feature_importance' (Dictionary of feature scores, if available).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any feature returned by `get_feature_list()` is missing from the input\n",
        "        DataFrame or if an unknown `model_type` is specified.\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"STEP 5: MODEL TRAINING\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "    feature_cols = get_feature_list()\n",
        "\n",
        "    # Check all features exist\n",
        "    missing_features = [f for f in feature_cols if f not in df.columns]\n",
        "    if missing_features:\n",
        "        raise ValueError(f\"Missing features: {missing_features}\")\n",
        "\n",
        "    X = df[feature_cols]\n",
        "    y = df['price']\n",
        "\n",
        "    # Handle any remaining NaN\n",
        "    if X.isnull().any().any():\n",
        "        if verbose:\n",
        "            print(\"‚ö†Ô∏è  Warning: Filling remaining NaN values\")\n",
        "        X = X.fillna(X.mean())\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nTraining data:\")\n",
        "        print(f\"  Samples: {len(X)}\")\n",
        "        print(f\"  Features: {len(feature_cols)}\")\n",
        "        print(f\"  Price range: ${y.min():.2f} - ${y.max():.2f}\")\n",
        "        print(f\"  Price mean: ${y.mean():.2f}\")\n",
        "        print(f\"  Price std: ${y.std():.2f}\")\n",
        "\n",
        "    # Build model\n",
        "    if model_type == 'lightgbm':\n",
        "        model = lgb.LGBMRegressor(\n",
        "            n_estimators=150,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=6,\n",
        "            num_leaves=25,\n",
        "            min_child_samples=15,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            reg_alpha=0.1,\n",
        "            reg_lambda=0.1,\n",
        "            random_state=42,\n",
        "            verbose=-1\n",
        "        )\n",
        "    elif model_type == 'gradient_boosting':\n",
        "        model = GradientBoostingRegressor(\n",
        "            n_estimators=150,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=6,\n",
        "            min_samples_split=15,\n",
        "            subsample=0.8,\n",
        "            random_state=42\n",
        "        )\n",
        "    elif model_type == 'random_forest':\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=150,\n",
        "            max_depth=10,\n",
        "            min_samples_split=10,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # Time series cross-validation\n",
        "    n_splits = min(5, len(df) // 50)  # At least 50 samples per fold\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nCross-validation with {n_splits} folds...\")\n",
        "\n",
        "    cv_scores = []\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "        mae = mean_absolute_error(y_val, y_pred)\n",
        "        r2 = r2_score(y_val, y_pred)\n",
        "        mape = mean_absolute_percentage_error(y_val, y_pred) * 100\n",
        "\n",
        "        cv_scores.append({'rmse': rmse, 'mae': mae, 'r2': r2, 'mape': mape})\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  Fold {fold}: MAE=${mae:.2f}, MAPE={mape:.1f}%, R¬≤={r2:.3f}\")\n",
        "\n",
        "    # Final model on all data\n",
        "    model.fit(X, y)\n",
        "    y_pred_final = model.predict(X)\n",
        "\n",
        "    final_metrics = {\n",
        "        'rmse': np.sqrt(mean_squared_error(y, y_pred_final)),\n",
        "        'mae': mean_absolute_error(y, y_pred_final),\n",
        "        'r2': r2_score(y, y_pred_final),\n",
        "        'mape': mean_absolute_percentage_error(y, y_pred_final) * 100\n",
        "    }\n",
        "\n",
        "    avg_cv_metrics = {\n",
        "        'rmse': np.mean([s['rmse'] for s in cv_scores]),\n",
        "        'mae': np.mean([s['mae'] for s in cv_scores]),\n",
        "        'r2': np.mean([s['r2'] for s in cv_scores]),\n",
        "        'mape': np.mean([s['mape'] for s in cv_scores])\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"MODEL PERFORMANCE\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"\\nCross-Validation (Out-of-Sample):\")\n",
        "        print(f\"  MAE:  ${avg_cv_metrics['mae']:.2f}\")\n",
        "        print(f\"  RMSE: ${avg_cv_metrics['rmse']:.2f}\")\n",
        "        print(f\"  MAPE: {avg_cv_metrics['mape']:.1f}%\")\n",
        "        print(f\"  R¬≤:   {avg_cv_metrics['r2']:.3f}\")\n",
        "\n",
        "        print(f\"\\nTraining Set:\")\n",
        "        print(f\"  MAE:  ${final_metrics['mae']:.2f}\")\n",
        "        print(f\"  MAPE: {final_metrics['mape']:.1f}%\")\n",
        "        print(f\"  R¬≤:   {final_metrics['r2']:.3f}\")\n",
        "\n",
        "        # Feature importance\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = sorted(zip(feature_cols, model.feature_importances_),\n",
        "                               key=lambda x: x[1], reverse=True)\n",
        "            print(f\"\\nTop 10 Features:\")\n",
        "            for i, (feat, imp) in enumerate(importances[:10], 1):\n",
        "                print(f\"  {i:2d}. {feat:30s} {imp:.4f}\")\n",
        "\n",
        "    # Calculate baseline stats\n",
        "    baseline_stats = {\n",
        "        'mean': y.mean(),\n",
        "        'std': y.std(),\n",
        "        'min': y.min(),\n",
        "        'max': y.max(),\n",
        "        'median': y.median()\n",
        "    }\n",
        "\n",
        "    # Store model artifacts\n",
        "    model_artifacts = {\n",
        "        'model': model,\n",
        "        'model_type': model_type,\n",
        "        'feature_cols': feature_cols,\n",
        "        'baseline_stats': baseline_stats,\n",
        "        'cv_metrics': avg_cv_metrics,\n",
        "        'train_metrics': final_metrics,\n",
        "        'training_samples': len(X),\n",
        "        'feature_importance': dict(zip(feature_cols, model.feature_importances_)) if hasattr(model, 'feature_importances_') else {}\n",
        "    }\n",
        "\n",
        "    return model_artifacts"
      ],
      "metadata": {
        "id": "ZICQ5ZL5mxjs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ STEP 6 & 7: Prediction and Evaluation\n",
        "\n",
        "Once the model is trained, the next steps involve generating predictions and rigorously quantifying the model's performance. This ensures the predictions are accurate before they are used to make business decisions (deal detection).\n",
        "\n",
        "-----\n",
        "\n",
        "## 6\\. Generating Predictions (`predict_prices`)\n",
        "\n",
        "This function uses the saved model to generate forecasts and performs a critical post-processing step: **clipping**.\n",
        "\n",
        "### The Clipping Mechanism\n",
        "\n",
        "The model's predictions are clipped to a reasonable range based on the **baseline statistics** of the target variable (price). This prevents the model from generating financially impossible or \"wild\" forecasts, especially when encountering extreme or unusual feature combinations in new data.\n",
        "\n",
        "$$\n",
        "\\text{Lower Bound} = \\text{Mean Price} \\times 0.4\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Upper Bound} = \\text{Mean Price} \\times 2.5\n",
        "$$\n",
        "\n",
        "Any prediction falling outside this range is forced to the corresponding boundary.\n",
        "\n",
        "### Calculated Error Metrics\n",
        "\n",
        "The output DataFrame is augmented with the core components required for evaluation and deal detection:\n",
        "\n",
        "| Column Name | Calculation | Purpose |\n",
        "| :--- | :--- | :--- |\n",
        "| `predicted_price` | `model.predict(X)` | The model's raw forecast. |\n",
        "| `prediction_error` | `price - predicted_price` | The absolute difference (in dollars) between actual and predicted price. |\n",
        "| `prediction_error_pct` | $(\\text{error} / \\text{price}) \\times 100$ | The percentage difference, which is the primary driver for deal detection. |\n",
        "\n",
        "-----\n",
        "\n",
        "## 7\\. Model Evaluation (`evaluate_model`)\n",
        "\n",
        "The `evaluate_model` function provides a comprehensive performance review by calculating key regression metrics.\n",
        "\n",
        "### Key Regression Metrics\n",
        "\n",
        "- **MAE** (Mean Absolute Error)\n",
        "- **RMSE** (Root Mean Squared Error)\n",
        "- **MAPE** (Mean Absolute % Error)\n",
        "- **R¬≤** (Coefficient of Determination)\n",
        "\n",
        "### Code\n",
        "\n",
        "```python\n",
        "def evaluate_model(df_with_predictions: pd.DataFrame, verbose: bool = True) -> Dict[str, float]:\n",
        "    # ... implementation details\n",
        "    # Example output:\n",
        "    # MAE: $15.50\n",
        "    # MAPE: 7.2%\n",
        "    # R¬≤: 0.945\n",
        "    return metrics\n",
        "```"
      ],
      "metadata": {
        "id": "JJJJQvpcwKUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_prices(model_artifacts: Dict[str, Any], df: pd.DataFrame,\n",
        "                  verbose: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates price predictions on new, unseen, or validation data using the trained model.\n",
        "\n",
        "    The function applies necessary preprocessing (feature selection, NaN handling)\n",
        "    and prediction, includes a clipping mechanism to ensure predictions remain\n",
        "    within a reasonable, bounds-checked range, and appends the predictions and\n",
        "    error metrics back to the input DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_artifacts : Dict[str, Any]\n",
        "        A dictionary containing the trained model and associated metadata,\n",
        "        typically the output of the `train_price_model` function. Must include\n",
        "        'model' (the fitted model object), 'feature_cols' (list of features\n",
        "        used in training), and 'baseline_stats' (for clipping bounds).\n",
        "    df : pd.DataFrame\n",
        "        The DataFrame containing the new data to make predictions on. Must\n",
        "        contain all columns specified in `feature_cols`.\n",
        "    verbose : bool, optional\n",
        "        If True, prints a summary of the prediction run, including the total\n",
        "        number of predictions and the average prediction error statistics.\n",
        "        The default is False.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        A copy of the input DataFrame augmented with three new columns:\n",
        "        'predicted_price', 'prediction_error', and 'prediction_error_pct'.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    KeyError\n",
        "        If required keys ('model', 'feature_cols', 'baseline_stats') are missing\n",
        "        from `model_artifacts`.\n",
        "    \"\"\"\n",
        "    model = model_artifacts['model']\n",
        "    feature_cols = model_artifacts['feature_cols']\n",
        "    baseline = model_artifacts['baseline_stats']\n",
        "\n",
        "    X = df[feature_cols]\n",
        "\n",
        "    # Handle NaN\n",
        "    if X.isnull().any().any():\n",
        "        X = X.fillna(X.mean())\n",
        "\n",
        "    # Predict\n",
        "    predictions = model.predict(X)\n",
        "\n",
        "    # Clip to reasonable range (prevent wild predictions)\n",
        "    lower_bound = baseline['mean'] * 0.4\n",
        "    upper_bound = baseline['mean'] * 2.5\n",
        "    predictions = np.clip(predictions, lower_bound, upper_bound)\n",
        "\n",
        "    # Add to dataframe\n",
        "    result = df.copy()\n",
        "    result['predicted_price'] = predictions\n",
        "    result['prediction_error'] = result['price'] - result['predicted_price']\n",
        "    result['prediction_error_pct'] = (result['prediction_error'] / result['price']) * 100\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nPredictions generated for {len(result)} offers\")\n",
        "        print(f\"Average error: ${result['prediction_error'].abs().mean():.2f}\")\n",
        "        print(f\"Average error %: {result['prediction_error_pct'].abs().mean():.1f}%\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def evaluate_model(df_with_predictions: pd.DataFrame, verbose: bool = True) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculates and reports key regression metrics to evaluate the model's predictive performance.\n",
        "\n",
        "    The evaluation uses standard metrics that quantify both the magnitude of the\n",
        "    errors (MAE, RMSE, Median Error) and the model's overall fit (R¬≤), as well\n",
        "    as an easily interpretable percentage error (MAPE).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_with_predictions : pd.DataFrame\n",
        "        The DataFrame containing both the true 'price' (actual target value) and\n",
        "        the 'predicted_price' columns, typically the output of the\n",
        "        `predict_prices` function.\n",
        "    verbose : bool, optional\n",
        "        If True, prints a header and a formatted summary of all calculated\n",
        "        metrics to the console. The default is True.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, float]\n",
        "        A dictionary containing the calculated evaluation metrics:\n",
        "        - 'mae' (Mean Absolute Error, in currency units)\n",
        "        - 'rmse' (Root Mean Squared Error, in currency units)\n",
        "        - 'mape' (Mean Absolute Percentage Error, as a percentage)\n",
        "        - 'r2' (Coefficient of Determination, unitless)\n",
        "        - 'median_error' (Median Absolute Error, in currency units)\n",
        "    \"\"\"\n",
        "    actual = df_with_predictions['price']\n",
        "    predicted = df_with_predictions['predicted_price']\n",
        "\n",
        "    metrics = {\n",
        "        'mae': mean_absolute_error(actual, predicted),\n",
        "        'rmse': np.sqrt(mean_squared_error(actual, predicted)),\n",
        "        'mape': mean_absolute_percentage_error(actual, predicted) * 100,\n",
        "        'r2': r2_score(actual, predicted),\n",
        "        'median_error': df_with_predictions['prediction_error'].abs().median(),\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"MAE:  ${metrics['mae']:.2f}\")\n",
        "        print(f\"RMSE: ${metrics['rmse']:.2f}\")\n",
        "        print(f\"MAPE: {metrics['mape']:.1f}%\")\n",
        "        print(f\"R¬≤:   {metrics['r2']:.3f}\")\n",
        "        print(f\"Median Absolute Error: ${metrics['median_error']:.2f}\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "ZPCiQQQdoA7b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèÜ STEP 8: Deal Detection and Actionable Insights\n",
        "\n",
        "The final step of the pipeline translates the model's prediction error into a clear, actionable recommendation. This is the **business intelligence layer** of the forecasting solution.\n",
        "\n",
        "-----\n",
        "\n",
        "## `detect_deals` Function\n",
        "\n",
        "The function categorizes every flight offer based on its `prediction_error_pct`‚Äîthe percentage difference between the actual observed price and the model's predicted fair market price.\n",
        "\n",
        "A **negative error percentage** means the actual price is *below* the predicted price, indicating a potential deal.\n",
        "\n",
        "### Deal Classification Logic\n",
        "\n",
        "| Category | `prediction_error_pct` | Recommendation | Actionable Insight |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **EXCELLENT** | $\\le \\text{excellent\\_threshold}$ (e.g., $-12.0\\%$) | **üî• BUY NOW** | Price is significantly below the fair market value. |\n",
        "| **GOOD** | $> \\text{excellent\\_threshold}$ and $\\le \\text{good\\_threshold}$ (e.g., $-12.0\\%$ to $-7.0\\%$) | **‚úÖ STRONG BUY** | Price is moderately below the fair market value. |\n",
        "| **FAIR** | Between thresholds (e.g., $-7.0\\%$ to $+10.0\\%$) | **‚ûñ NEUTRAL** | Price is within the expected range‚Äîpurchase if needed. |\n",
        "| **OVERPRICED** | $\\ge \\text{overpriced\\_threshold}$ (e.g., $+10.0\\%$) | **‚ùå WAIT** | Price is significantly above the fair market value; wait for a potential drop. |\n",
        "\n",
        "This process transforms a technical output (a percentage error) into an intuitive, final decision column (`recommendation`) for end-users or stakeholders.\n",
        "\n",
        "### Code\n",
        "\n",
        "```python\n",
        "def detect_deals(df_with_predictions: pd.DataFrame,\n",
        "                 excellent_threshold: float = -12.0,\n",
        "                 good_threshold: float = -7.0,\n",
        "                 overpriced_threshold: float = 10.0,\n",
        "                 verbose: bool = True) -> pd.DataFrame:\n",
        "    # ... implementation details\n",
        "    return df\n",
        "```"
      ],
      "metadata": {
        "id": "48u1-yOoxxjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_deals(df_with_predictions: pd.DataFrame,\n",
        "                excellent_threshold: float = -12.0,\n",
        "                good_threshold: float = -7.0,\n",
        "                overpriced_threshold: float = 10.0,\n",
        "                verbose: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Categorizes flight offers as 'EXCELLENT', 'GOOD', 'FAIR', or 'OVERPRICED'\n",
        "    based on the percentage difference between the actual observed price and the\n",
        "    model's predicted price.\n",
        "\n",
        "    This function is the final step in the pipeline, translating the model's\n",
        "    output into actionable business intelligence (i.e., buy/wait recommendations).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_with_predictions : pd.DataFrame\n",
        "        The DataFrame containing flight offers, including the 'price' (actual)\n",
        "        and 'predicted_price' columns, typically the output of the\n",
        "        `predict_prices` function. Must contain the 'prediction_error_pct' column.\n",
        "    excellent_threshold : float, optional\n",
        "        The percentage error threshold (e.g., -12.0) below which an offer is\n",
        "        classified as 'EXCELLENT' (Actual Price <= Predicted Price - 12%).\n",
        "        The default is -12.0.\n",
        "    good_threshold : float, optional\n",
        "        The percentage error threshold (e.g., -7.0) below which an offer is\n",
        "        classified as 'GOOD' (Actual Price <= Predicted Price - 7%).\n",
        "        Must be less than `overpriced_threshold`. The default is -7.0.\n",
        "    overpriced_threshold : float, optional\n",
        "        The positive percentage error threshold (e.g., 10.0) above which an offer\n",
        "        is classified as 'OVERPRICED' (Actual Price >= Predicted Price + 10%).\n",
        "        The default is 10.0.\n",
        "    verbose : bool, optional\n",
        "        If True, prints a summary of the deal distribution and a list of the\n",
        "        top 5 best deals found. The default is True.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The input DataFrame augmented with two new columns: 'deal_category'\n",
        "        (the qualitative classification) and 'recommendation' (the actionable\n",
        "        advice, e.g., 'üî• BUY NOW').\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    A large negative error means the actual price is far below the predicted (a good deal).\n",
        "    \"\"\"\n",
        "    df = df_with_predictions.copy()\n",
        "\n",
        "    # Calculate deal category\n",
        "    df['deal_category'] = 'FAIR'\n",
        "    df.loc[df['prediction_error_pct'] <= excellent_threshold, 'deal_category'] = 'EXCELLENT'\n",
        "    df.loc[(df['prediction_error_pct'] > excellent_threshold) &\n",
        "           (df['prediction_error_pct'] <= good_threshold), 'deal_category'] = 'GOOD'\n",
        "    df.loc[df['prediction_error_pct'] >= overpriced_threshold, 'deal_category'] = 'OVERPRICED'\n",
        "\n",
        "    # Add recommendations\n",
        "    recommendations = {\n",
        "        'EXCELLENT': 'üî• BUY NOW',\n",
        "        'GOOD': '‚úÖ STRONG BUY',\n",
        "        'FAIR': '‚ûñ NEUTRAL',\n",
        "        'OVERPRICED': '‚ùå WAIT'\n",
        "    }\n",
        "\n",
        "    df['recommendation'] = df['deal_category'].map(recommendations)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"DEAL DETECTION SUMMARY\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"\\nDeal distribution:\")\n",
        "        print(df['deal_category'].value_counts())\n",
        "        print(f\"\\nBest deals (top 5):\")\n",
        "        best = df.nsmallest(5, 'prediction_error_pct')[\n",
        "            ['departure_date', 'primary_airline', 'price', 'predicted_price',\n",
        "             'prediction_error_pct', 'deal_category']\n",
        "        ]\n",
        "        print(best.to_string(index=False))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "4o9Gs3VWpA6-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üíæ STEP 9: Model Persistence (Save & Load)\n",
        "\n",
        "The final step in the modeling pipeline is to **persist** the trained model and all associated artifacts to disk. This is crucial for **deployment**\n",
        "\n",
        ", as it allows the entire forecasting system to be reloaded and used for new predictions without needing to retrain the model every time.\n",
        "\n",
        "-----\n",
        "\n",
        "## `save_model`\n",
        "\n",
        "The `save_model` function uses Python's built-in **`pickle`** module to serialize the entire `model_artifacts` dictionary‚Äîwhich includes the fitted LightGBM object, feature list, and performance metrics‚Äîinto a binary file (typically ending in `.pkl`).\n",
        "\n",
        "This single file contains everything needed to reproduce the model's predictions.\n",
        "\n",
        "### Key Details\n",
        "\n",
        "  * **Mechanism:** `pickle.dump()`\n",
        "  * **Purpose:** To turn the Python object (the model dictionary) into a byte stream for storage.\n",
        "  * **Input:** The full artifact dictionary from the training step.\n",
        "  * **Output:** A `.pkl` file on disk.\n",
        "\n",
        "<!-- end list -->\n",
        "\n",
        "```python\n",
        "def save_model(model_artifacts: Dict[str, Any], filepath: str):\n",
        "    # ... implementation details\n",
        "    print(f\"\\n‚úì Model saved to {filepath}\")\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "## `load_model`\n",
        "\n",
        "The `load_model` function performs the reverse operation, reading the binary `.pkl` file from disk and reconstructing the Python object in memory.\n",
        "\n",
        "### Key Details\n",
        "\n",
        "  * **Mechanism:** `pickle.load()`\n",
        "  * **Purpose:** To deserialize the byte stream back into the original Python dictionary object.\n",
        "  * **Input:** The file path of the `.pkl` file.\n",
        "  * **Output:** The complete `model_artifacts` dictionary, ready to be passed to the `predict_prices` function.\n",
        "\n",
        "<!-- end list -->\n",
        "\n",
        "```python\n",
        "def load_model(filepath: str) -> Dict[str, Any]:\n",
        "    # ... implementation details\n",
        "    print(f\"‚úì Model loaded from {filepath}\")\n",
        "    return model_artifacts\n",
        "```"
      ],
      "metadata": {
        "id": "Lk_dbRSbyEHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SAVE AND LOAD FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def save_model(model_artifacts: Dict[str, Any], filepath: str):\n",
        "    \"\"\"\n",
        "    Saves the complete dictionary of model artifacts (including the fitted model\n",
        "    object, features, and metrics) to disk using Python's `pickle` module.\n",
        "\n",
        "    This ensures the model can be persisted and later reloaded for prediction\n",
        "    or deployment without needing to retrain it.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_artifacts : Dict[str, Any]\n",
        "        A dictionary containing all components necessary for prediction,\n",
        "        typically the output of the `train_price_model` function. This dictionary\n",
        "        must contain the fitted model object itself.\n",
        "    filepath : str\n",
        "        The full path and filename (including the extension, e.g., '.pkl')\n",
        "        where the model artifacts should be saved.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        Prints a confirmation message upon successful saving.\n",
        "    \"\"\"\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(model_artifacts, f)\n",
        "    print(f\"\\n‚úì Model saved to {filepath}\")\n",
        "\n",
        "\n",
        "def load_model(filepath: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Loads the complete dictionary of model artifacts from a file on disk\n",
        "    that was previously saved using `pickle`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : str\n",
        "        The full path and filename of the pickled model artifact file (e.g., '.pkl').\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        The loaded dictionary containing the fitted model, feature columns,\n",
        "        and performance metrics. Prints a confirmation message upon successful loading.\n",
        "    \"\"\"\n",
        "    with open(filepath, 'rb') as f:\n",
        "        model_artifacts = pickle.load(f)\n",
        "    print(f\"‚úì Model loaded from {filepath}\")\n",
        "    return model_artifacts\n"
      ],
      "metadata": {
        "id": "XgRTih-orB49",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üõ†Ô∏è COMPLETE PIPELINE ORCHESTRATION\n",
        "\n",
        "The `run_complete_pipeline` function serves as the **master executive script**, coordinating all data science and machine learning tasks from raw data ingestion to final model persistence and deal recommendation. This function ensures that every step is executed sequentially, passing the appropriate data and artifacts to the next stage.\n",
        "\n",
        "-----\n",
        "\n",
        "## `run_complete_pipeline` Function\n",
        "\n",
        "This function executes the complete machine learning workflow for the Flight Price Forecasting project.\n",
        "\n",
        "### Workflow Summary\n",
        "\n",
        "| Step \\# | Action | Function Called | Key Outcome |\n",
        "| :---: | :--- | :--- | :--- |\n",
        "| **1** | Data Ingestion & Cleaning | `get_data`, `clean_raw_data` | Unified, journey-level flight records. |\n",
        "| **2** | Data Filtering | `filter_target_route` | Focus on target origin/destination pairs (e.g., IAH ‚Üí LAX/ONT). |\n",
        "| **3** | Standard Feature Engineering | `engineer_features` | Creation of temporal, flight, and booking window features. |\n",
        "| **4** | Time-Series Features | `create_price_history_features` | Generation of lagged prices, rolling means, and price change features. |\n",
        "| **5** | Model Training | `train_price_model` | A fully fitted LightGBM model object. |\n",
        "| **6** | Prediction | `predict_prices` | DataFrame augmented with `predicted_price` and error columns. |\n",
        "| **7** | Evaluation | `evaluate_model` | Printed performance metrics (MAE, MAPE, R¬≤). |\n",
        "| **8** | Deal Detection | `detect_deals` | Final, actionable DataFrame with `deal_category` and `recommendation`. |\n",
        "| **9** | Model Saving | `save_model` | Persisted model artifacts on disk (`flight_model.pkl`). |\n",
        "\n",
        "### Code\n",
        "\n",
        "```python\n",
        "def run_complete_pipeline(raw_data_path: str, save_path: str = 'flight_model.pkl'):\n",
        "    \"\"\"\n",
        "    Executes the complete flight price forecasting machine learning pipeline\n",
        "    from raw data ingestion through model training, evaluation, deal detection,\n",
        "    and final model serialization.\n",
        "    \"\"\"\n",
        "    # ... implementation details ...\n",
        "    \n",
        "    # Example Call Sequence:\n",
        "    # df_raw = get_data(data_path)\n",
        "    # df_clean = clean_raw_data(df_raw, verbose=True)\n",
        "    # ...\n",
        "    # model_artifacts = train_price_model(df_final, model_type='lightgbm', verbose=True)\n",
        "    # ...\n",
        "    # save_model(model_artifacts, save_path)\n",
        "    \n",
        "    return model_artifacts, df_deals\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "### Final Output\n",
        "\n",
        "The function concludes by printing a **PIPELINE COMPLETE** message and returns the two core outputs:\n",
        "\n",
        "1.  **`model_artifacts`**: The complete trained model and its metadata for deployment.\n",
        "2.  **`df_deals`**: The finalized dataset containing all the data, the model's predictions, and the recommended purchasing action for each flight offer."
      ],
      "metadata": {
        "id": "EE0f2WOeyT2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# COMPLETE PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def run_complete_pipeline(raw_data_path: str, save_path: str = 'flight_model.pkl'):\n",
        "    \"\"\"\n",
        "    Executes the complete flight price forecasting machine learning pipeline\n",
        "    from raw data ingestion through model training, evaluation, deal detection,\n",
        "    and final model serialization.\n",
        "\n",
        "    This function coordinates all preceding steps: cleaning, filtering, feature\n",
        "    engineering (including creation of price history features), training a\n",
        "    LightGBM model using time-series cross-validation, applying the model to\n",
        "    detect deals, and saving the resulting artifacts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw_data_path : str\n",
        "        The path to the initial raw data file (although the implementation\n",
        "        uses a pre-loaded variable `all_data` for demonstration, this parameter\n",
        "        indicates the intended source).\n",
        "    save_path : str, optional\n",
        "        The path and filename where the trained model and associated artifacts\n",
        "        will be pickled and saved. The default is 'flight_model.pkl'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[Dict[str, Any], pd.DataFrame]\n",
        "        A tuple containing:\n",
        "        1. model_artifacts (Dict[str, Any]): The dictionary containing the\n",
        "           fitted model, feature list, and performance metrics.\n",
        "        2. df_deals (pd.DataFrame): The final DataFrame with all features,\n",
        "           actual prices, predicted prices, error metrics, and the final\n",
        "           'deal_category' and 'recommendation' columns.\n",
        "\n",
        "    Process Steps\n",
        "    -------------\n",
        "    1. Load and Clean Data: Standardize data types, handle missing values.\n",
        "    2. Filter Routes: Select target origin-destination pairs (e.g., IAH -> LAX/ONT).\n",
        "    3. Engineer Features: Create temporal, booking window, and flight characteristic features.\n",
        "    4. Create Price History Features: Generate lag, rolling mean, and price momentum features.\n",
        "    5. Train Model: Fit the LightGBM model using TimeSeriesSplit cross-validation.\n",
        "    6. Predict Prices: Generate predictions on the training dataset.\n",
        "    7. Evaluate Model: Calculate and print metrics (MAE, RMSE, MAPE, R¬≤).\n",
        "    8. Detect Deals: Classify offers based on the error percentage (e.g., 'EXCELLENT', 'OVERPRICED').\n",
        "    9. Save Model: Persist the model artifacts to disk.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'#'*70}\")\n",
        "    print(\"FLIGHT PRICE FORECASTING PIPELINE\")\n",
        "    print(f\"{'#'*70}\")\n",
        "\n",
        "    # Step 1: Load and clean\n",
        "    print(\"\\nLoading raw data...\")\n",
        "\n",
        "    df_raw = get_data(raw_data_path)\n",
        "    df_clean = clean_raw_data(df_raw, verbose=True)\n",
        "\n",
        "    # Step 2: Filter routes\n",
        "    df_filtered = filter_target_route(df_clean, verbose=True)\n",
        "\n",
        "    # Step 3: Engineer features\n",
        "    df_features = engineer_features(df_filtered, verbose=True)\n",
        "    df_final = create_price_history_features(df_features, verbose=True)\n",
        "\n",
        "    # Step 4: Train model\n",
        "    model_artifacts = train_price_model(df_final, model_type='lightgbm', verbose=True)\n",
        "\n",
        "    # Step 5: Make predictions on training data\n",
        "    df_with_pred = predict_prices(model_artifacts, df_final, verbose=True)\n",
        "\n",
        "    # Step 6: Evaluate\n",
        "    evaluate_model(df_with_pred, verbose=True)\n",
        "\n",
        "    # Step 7: Detect deals\n",
        "    df_deals = detect_deals(df_with_pred, verbose=True)\n",
        "\n",
        "    # Step 8: Save model\n",
        "    save_model(model_artifacts, save_path)\n",
        "\n",
        "    print(f\"\\n{'#'*70}\")\n",
        "    print(\"PIPELINE COMPLETE\")\n",
        "    print(f\"{'#'*70}\")\n",
        "\n",
        "    return model_artifacts, df_deals\n"
      ],
      "metadata": {
        "id": "239K_e2wp78O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ EXAMPLE USAGE\n",
        "\n",
        "This final block demonstrates how to execute the entire end-to-end pipeline and, critically, how to use the saved model for future predictions in a deployment scenario\n",
        "\n",
        "-----\n",
        "\n",
        "## 1\\. Running the Complete Pipeline\n",
        "\n",
        "The `if __name__ == \"__main__\":` block is the standard entry point in Python for running scripts. It orchestrates the entire process from data ingestion through training and saving.\n",
        "\n",
        "```python\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    model, results = run_complete_pipeline(\n",
        "        raw_data_path='Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw',\n",
        "        save_path='flight_price_model_v3.pkl'\n",
        "    )\n",
        "    \n",
        "    print(\"\\n‚úì Ready for deployment!\")\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "## 2\\. Deployment Flow (Inference on New Data)\n",
        "\n",
        "Once the model has been trained and saved, the pipeline transitions from **training mode** to **inference mode**.\n",
        "\n",
        "For new, incoming daily data, the model does *not* need to be retrained. Instead, the new data must be processed through the exact same feature engineering steps used during training before generating predictions.\n",
        "\n",
        "| Step | Action | Function | Rationale |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **1. Load Model** | Retrieve the trained model object. | `load_model()` | Uses the saved `.pkl` file. |\n",
        "| **2. Clean Data** | Consolidate segments into journeys. | `clean_raw_data()` | Maintains data integrity and granularity. |\n",
        "| **3. Filter** | Focus on the modeled route(s). | `filter_target_route()` | Ensures consistency with training data scope. |\n",
        "| **4. Features** | Create temporal/flight features. | `engineer_features()` | Adds simple, static features. |\n",
        "| **5. History** | Create price lag/rolling stats. | `create_price_history_features()` | **Crucial:** Recalculates time-series features based on the history up to the current day. |\n",
        "| **6. Predict** | Generate the forecast. | `predict_prices()` | Applies the fitted model. |\n",
        "| **7. Deals** | Classify the offers. | `detect_deals()` | Translates prediction error into actionable advice (e.g., **üî• BUY NOW**). |\n",
        "\n",
        "```python\n",
        "print(\"To use the model:\")\n",
        "print(\"   1. Load: model = load_model('flight_price_model_v3.pkl')\")\n",
        "print(\"   2. Clean new data: df_clean = clean_raw_data(df_raw)\")\n",
        "print(\"   3. Filter: df_filtered = filter_target_route(df_clean)\")\n",
        "print(\"   4. Features: df_features = engineer_features(df_filtered)\")\n",
        "print(\"   5. History: df_final = create_price_history_features(df_features)\")\n",
        "print(\"   6. Predict: df_pred = predict_prices(model, df_final)\")\n",
        "print(\"   7. Deals: df_deals = detect_deals(df_pred)\")\n",
        "```"
      ],
      "metadata": {
        "id": "LI7cXCTzy7ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# EXAMPLE USAGE\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    model, results = run_complete_pipeline(\n",
        "        raw_data_path='Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw',\n",
        "        save_path='flight_price_model_v3.pkl'\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚úì Ready for deployment!\")\n",
        "    print(\"\\nTo use the model:\")\n",
        "    print(\"  1. Load: model = load_model('flight_price_model_v3.pkl')\")\n",
        "    print(\"  2. Clean new data: df_clean = clean_raw_data(df_raw)\")\n",
        "    print(\"  3. Filter: df_filtered = filter_target_route(df_clean)\")\n",
        "    print(\"  4. Features: df_features = engineer_features(df_filtered)\")\n",
        "    print(\"  5. History: df_final = create_price_history_features(df_features)\")\n",
        "    print(\"  6. Predict: df_pred = predict_prices(model, df_final)\")\n",
        "    print(\"  7. Deals: df_deals = detect_deals(df_pred)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leKXI0RqqBX3",
        "outputId": "29b97ded-e1cc-4e61-e765-98f6b532087a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######################################################################\n",
            "FLIGHT PRICE FORECASTING PIPELINE\n",
            "######################################################################\n",
            "\n",
            "Loading raw data...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-03.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-04.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-05.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-06.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-07.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-08.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-09.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-10.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-11.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-12.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-13.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-14.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-15.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-16.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-17.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-18.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-19.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-20.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-21.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-22.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-23.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-24.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-25.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-26.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-27.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-28.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-29.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-30.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-10-31.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-01.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-02.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-03.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-04.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-05.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-06.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-07.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-08.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-09.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-10.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-11.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-12.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-13.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-14.csv...\n",
            "Loading Airline-Flight-Price-Analysis-with-APIs-Azure/data/raw/2025-11-15.csv...\n",
            "Loaded 44 files, 14800 rows\n",
            "\n",
            "======================================================================\n",
            "STEP 1: DATA CLEANING\n",
            "======================================================================\n",
            "Raw data shape: (14800, 11)\n",
            "\n",
            "Cleaned data shape: (10340, 15)\n",
            "Unique offers: 5\n",
            "Price range: $26.09 - $390.51\n",
            "Date range: 2025-10-03 to 2025-11-15\n",
            "\n",
            "Flight types:\n",
            "  Direct flights: 5880 (56.9%)\n",
            "  Connecting flights: 4460 (43.1%)\n",
            "\n",
            "Airlines:\n",
            "primary_airline\n",
            "NK    2944\n",
            "UA    2648\n",
            "F9    2609\n",
            "AS    2139\n",
            "Name: count, dtype: int64\n",
            "\n",
            "======================================================================\n",
            "STEP 2: ROUTE FILTERING\n",
            "======================================================================\n",
            "Filtering: IAH ‚Üí ['LAX', 'ONT']\n",
            "Before: 10340 offers\n",
            "After: 10340 offers\n",
            "\n",
            "Destination breakdown:\n",
            "destination\n",
            "LAX    8388\n",
            "ONT    1952\n",
            "Name: count, dtype: int64\n",
            "\n",
            "======================================================================\n",
            "STEP 3: FEATURE ENGINEERING\n",
            "======================================================================\n",
            "\n",
            "Features created: 19 engineered features\n",
            "\n",
            "Booking window distribution:\n",
            "  Last minute (‚â§7 days): 180\n",
            "  Moderate (8-29 days): 2145\n",
            "  Early (‚â•30 days): 8015\n",
            "\n",
            "======================================================================\n",
            "STEP 4: PRICE HISTORY FEATURES\n",
            "======================================================================\n",
            "Price history features created\n",
            "Average price: $109.54\n",
            "Price std dev: $57.38\n",
            "\n",
            "======================================================================\n",
            "STEP 5: MODEL TRAINING\n",
            "======================================================================\n",
            "\n",
            "Training data:\n",
            "  Samples: 10340\n",
            "  Features: 26\n",
            "  Price range: $26.09 - $390.51\n",
            "  Price mean: $109.54\n",
            "  Price std: $57.38\n",
            "\n",
            "Cross-validation with 5 folds...\n",
            "  Fold 1: MAE=$9.10, MAPE=7.2%, R¬≤=0.898\n",
            "  Fold 2: MAE=$7.63, MAPE=9.8%, R¬≤=0.889\n",
            "  Fold 3: MAE=$3.42, MAPE=5.8%, R¬≤=0.879\n",
            "  Fold 4: MAE=$6.28, MAPE=4.9%, R¬≤=0.929\n",
            "  Fold 5: MAE=$4.50, MAPE=3.5%, R¬≤=0.955\n",
            "\n",
            "======================================================================\n",
            "MODEL PERFORMANCE\n",
            "======================================================================\n",
            "\n",
            "Cross-Validation (Out-of-Sample):\n",
            "  MAE:  $6.19\n",
            "  RMSE: $14.90\n",
            "  MAPE: 6.2%\n",
            "  R¬≤:   0.910\n",
            "\n",
            "Training Set:\n",
            "  MAE:  $3.07\n",
            "  MAPE: 3.1%\n",
            "  R¬≤:   0.975\n",
            "\n",
            "Top 10 Features:\n",
            "   1. price_change_1d                850.0000\n",
            "   2. price_lag_1                    672.0000\n",
            "   3. days_until_departure           316.0000\n",
            "   4. price_min_last_7               254.0000\n",
            "   5. price_rolling_std_3            232.0000\n",
            "   6. duration_hours                 230.0000\n",
            "   7. price_rolling_mean_3           192.0000\n",
            "   8. price_max_last_7               124.0000\n",
            "   9. departure_day_of_week          108.0000\n",
            "  10. airline_encoded                107.0000\n",
            "\n",
            "Predictions generated for 10340 offers\n",
            "Average error: $4.14\n",
            "Average error %: 6.4%\n",
            "\n",
            "======================================================================\n",
            "EVALUATION RESULTS\n",
            "======================================================================\n",
            "MAE:  $4.14\n",
            "RMSE: $9.82\n",
            "MAPE: 6.4%\n",
            "R¬≤:   0.971\n",
            "Median Absolute Error: $1.17\n",
            "\n",
            "======================================================================\n",
            "DEAL DETECTION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Deal distribution:\n",
            "deal_category\n",
            "FAIR          8590\n",
            "EXCELLENT     1410\n",
            "OVERPRICED     207\n",
            "GOOD           133\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Best deals (top 5):\n",
            "departure_date primary_airline  price  predicted_price  prediction_error_pct deal_category\n",
            "    2025-12-15              F9  29.33        98.162352           -234.682413     EXCELLENT\n",
            "    2025-12-14              F9  29.33        97.534116           -232.540457     EXCELLENT\n",
            "    2025-12-08              F9  29.33        95.047729           -224.063175     EXCELLENT\n",
            "    2025-11-15              F9  29.33        92.581235           -215.653718     EXCELLENT\n",
            "    2025-12-13              F9  29.33        90.998070           -210.255951     EXCELLENT\n",
            "\n",
            "‚úì Model saved to flight_price_model_v3.pkl\n",
            "\n",
            "######################################################################\n",
            "PIPELINE COMPLETE\n",
            "######################################################################\n",
            "\n",
            "‚úì Ready for deployment!\n",
            "\n",
            "To use the model:\n",
            "  1. Load: model = load_model('flight_price_model_v3.pkl')\n",
            "  2. Clean new data: df_clean = clean_raw_data(df_raw)\n",
            "  3. Filter: df_filtered = filter_target_route(df_clean)\n",
            "  4. Features: df_features = engineer_features(df_filtered)\n",
            "  5. History: df_final = create_price_history_features(df_features)\n",
            "  6. Predict: df_pred = predict_prices(model, df_final)\n",
            "  7. Deals: df_deals = detect_deals(df_pred)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ Conclusion: From Code to Impact\n",
        "\n",
        "We have successfully constructed and validated a complete **Flight Price Forecasting Pipeline**. This project moved from messy, segment-level raw data through rigorous feature engineering and robust modeling, resulting in an actionable deal detection system.\n",
        "\n",
        "The core achievement of this pipeline is the model's ability to accurately predict the fair market price of a flight, quantified by its low **Mean Absolute Percentage Error (MAPE)** achieved via **Time Series Cross-Validation (TSCV)**. This level of accuracy allows the system to generate reliable, high-value recommendations: **\"üî• BUY NOW\"** or **\"‚ùå WAIT\"**.\n",
        "\n",
        "### Next Phase: Deployment to Azure üöÄ\n",
        "\n",
        "The next, and most critical, phase is to move this intelligence from the local notebook environment to a scalable, production-ready system. We will leverage **Microsoft Azure** to deploy the forecasting model and automate the deal detection process.\n",
        "\n",
        "The deployment will focus on two key components:\n",
        "\n",
        "1.  **Azure Machine Learning Service:** To run the **data preparation and feature engineering steps** (Steps 1-4) on new, incoming data daily.\n",
        "2.  **Azure App Service / Azure Functions:** To host the saved model (`flight_price_model_v3.pkl`) and expose an **API endpoint**. This endpoint will take new flight offers as input and return the `predicted_price`, `prediction_error`, and the final `recommendation` in real-time or near-real-time.\n",
        "\n",
        "By deploying on Azure, we transform this analysis into a continuously running service that can handle massive volumes of flight data and deliver timely insights to users.\n"
      ],
      "metadata": {
        "id": "jePYrtO9zeco"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8GMZ89ezlcV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}